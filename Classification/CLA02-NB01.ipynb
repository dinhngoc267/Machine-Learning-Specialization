{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment from product reviews\n",
    "\n",
    "\n",
    "The goal of this first notebook is to explore logistic regression and feature engineering with existing Turi Create functions.\n",
    "\n",
    "In this notebook you will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n",
    "\n",
    "* Use SFrames to do some feature engineering\n",
    "* Train a logistic regression model to predict the sentiment of product reviews.\n",
    "* Inspect the weights (coefficients) of a trained logistic regression model.\n",
    "* Make a prediction (both class and probability) of sentiment for a new product review.\n",
    "* Given the logistic regression weights, predictors and ground truth labels, write a function to compute the **accuracy** of the model.\n",
    "* Inspect the coefficients of the logistic regression model and interpret their meanings.\n",
    "* Compare multiple logistic regression models.\n",
    "\n",
    "Let's get started!\n",
    "    \n",
    "## Fire up Turi Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of Turi Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import string\n",
    "import pandas as pd\n",
    "import gdown\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gdown -q 189CTuY0QHPoGrwE0fRLnAqWaYfa5NT7j -O ./amazon_baby.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "We will use a dataset consisting of baby product reviews on Amazon.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv(\"amazon_baby.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see a preview of what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183526</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>Such a great idea! very handy to have and look...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183527</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>This product rocks!  It is a great blend of fu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183528</th>\n",
       "      <td>Abstract 2 PK Baby / Toddler Training Cup (Pink)</td>\n",
       "      <td>This item looks great and cool for my kids.......</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183529</th>\n",
       "      <td>Baby Food Freezer Tray - Bacteria Resistant, B...</td>\n",
       "      <td>I am extremely happy with this product. I have...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183530</th>\n",
       "      <td>Best 2 Pack Baby Car Shade for Kids - Window S...</td>\n",
       "      <td>I love this product very mush . I have bought ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183531 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "0                                Planetwise Flannel Wipes   \n",
       "1                                   Planetwise Wipe Pouch   \n",
       "2                     Annas Dream Full Quilt with 2 Shams   \n",
       "3       Stop Pacifier Sucking without tears with Thumb...   \n",
       "4       Stop Pacifier Sucking without tears with Thumb...   \n",
       "...                                                   ...   \n",
       "183526  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "183527  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "183528   Abstract 2 PK Baby / Toddler Training Cup (Pink)   \n",
       "183529  Baby Food Freezer Tray - Bacteria Resistant, B...   \n",
       "183530  Best 2 Pack Baby Car Shade for Kids - Window S...   \n",
       "\n",
       "                                                   review  rating  \n",
       "0       These flannel wipes are OK, but in my opinion ...       3  \n",
       "1       it came early and was not disappointed. i love...       5  \n",
       "2       Very soft and comfortable and warmer than it l...       5  \n",
       "3       This is a product well worth the purchase.  I ...       5  \n",
       "4       All of my kids have cried non-stop when I trie...       5  \n",
       "...                                                   ...     ...  \n",
       "183526  Such a great idea! very handy to have and look...       5  \n",
       "183527  This product rocks!  It is a great blend of fu...       5  \n",
       "183528  This item looks great and cool for my kids.......       5  \n",
       "183529  I am extremely happy with this product. I have...       5  \n",
       "183530  I love this product very mush . I have bought ...       5  \n",
       "\n",
       "[183531 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows which have missing review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#products = products[products['review'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182702, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#products.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the word count vector for each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore a specific example of a baby product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name      The First Years Massaging Action Teether\n",
       "review                    A favorite in our house!\n",
       "rating                                           5\n",
       "Name: 269, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.iloc[269]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
    "2. Transform the reviews into word-counts.\n",
    "\n",
    "**Aside**. In this notebook, we remove all punctuations for the sake of simplicity. A smarter approach to punctuations would preserve phrases such as \"I'd\", \"would've\", \"hadn't\" and so forth. See [this page](https://www.cis.upenn.edu/~treebank/tokenization.html) for an example of smart handling of punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "def remove_punctuation(text):\n",
    "    try: # python 2.x\n",
    "        text = text.translate(None, string.punctuation) \n",
    "    except: # python 3.x\n",
    "        translator = str(text).maketrans('', '', string.punctuation)\n",
    "        text = str(text).translate(translator)\n",
    "    return text\n",
    "\n",
    "review_without_punctuation = products['review'].apply(remove_punctuation)\n",
    "#products['word_count'] = turicreate.text_analytics.count_words(review_without_punctuation)\n",
    "products['review_clean'] = review_without_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "vectorizer.fit(review_without_punctuation)\n",
    "tokenizer = vectorizer.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_count(text, tokenizer):\n",
    "    tokens = tokenizer(text)\n",
    "    word_count = {}\n",
    "  \n",
    "    for token in tokens:\n",
    "        if token in word_count:\n",
    "            word_count[token] += 1\n",
    "        else:\n",
    "            word_count[token] = 1\n",
    "      \n",
    "    return word_count\n",
    "\n",
    "word_count = []\n",
    "for review in review_without_punctuation:\n",
    "    word_count.append(create_word_count(review, tokenizer))\n",
    "products['word_count'] = word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us explore what the sample example above looks like after these 2 transformations. Here, each entry in the **word_count** column is a dictionary where the key is the word and the value is a count of the number of times the word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 1, 'favorite': 1, 'in': 1, 'our': 1, 'house': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.iloc[269]['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentiments\n",
    "\n",
    "We will **ignore** all reviews with *rating = 3*, since they tend to have a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})  # fill in N/A's in the review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166752"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "len(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assign reviews with a rating of 4 or higher to be *positive* reviews, while the ones with rating of 2 or lower are *negative*. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "      <td>it came early and was not disappointed i love ...</td>\n",
       "      <td>{'it': 3, 'came': 1, 'early': 1, 'and': 3, 'wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>{'Very': 1, 'soft': 1, 'and': 2, 'comfortable'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a product well worth the purchase  I h...</td>\n",
       "      <td>{'This': 1, 'is': 4, 'a': 2, 'product': 2, 'we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>All of my kids have cried nonstop when I tried...</td>\n",
       "      <td>{'All': 1, 'of': 1, 'my': 1, 'kids': 2, 'have'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>When the Binky Fairy came to our house, we did...</td>\n",
       "      <td>5</td>\n",
       "      <td>When the Binky Fairy came to our house we didn...</td>\n",
       "      <td>{'When': 1, 'the': 5, 'Binky': 3, 'Fairy': 3, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183526</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>Such a great idea! very handy to have and look...</td>\n",
       "      <td>5</td>\n",
       "      <td>Such a great idea very handy to have and look ...</td>\n",
       "      <td>{'Such': 1, 'a': 1, 'great': 2, 'idea': 1, 've...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183527</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>This product rocks!  It is a great blend of fu...</td>\n",
       "      <td>5</td>\n",
       "      <td>This product rocks  It is a great blend of fun...</td>\n",
       "      <td>{'This': 1, 'product': 2, 'rocks': 1, 'It': 1,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183528</th>\n",
       "      <td>Abstract 2 PK Baby / Toddler Training Cup (Pink)</td>\n",
       "      <td>This item looks great and cool for my kids.......</td>\n",
       "      <td>5</td>\n",
       "      <td>This item looks great and cool for my kidsI kn...</td>\n",
       "      <td>{'This': 1, 'item': 1, 'looks': 1, 'great': 2,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183529</th>\n",
       "      <td>Baby Food Freezer Tray - Bacteria Resistant, B...</td>\n",
       "      <td>I am extremely happy with this product. I have...</td>\n",
       "      <td>5</td>\n",
       "      <td>I am extremely happy with this product I have ...</td>\n",
       "      <td>{'I': 9, 'am': 2, 'extremely': 1, 'happy': 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183530</th>\n",
       "      <td>Best 2 Pack Baby Car Shade for Kids - Window S...</td>\n",
       "      <td>I love this product very mush . I have bought ...</td>\n",
       "      <td>5</td>\n",
       "      <td>I love this product very mush  I have bought m...</td>\n",
       "      <td>{'I': 2, 'love': 1, 'this': 1, 'product': 1, '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166752 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "1                                   Planetwise Wipe Pouch   \n",
       "2                     Annas Dream Full Quilt with 2 Shams   \n",
       "3       Stop Pacifier Sucking without tears with Thumb...   \n",
       "4       Stop Pacifier Sucking without tears with Thumb...   \n",
       "5       Stop Pacifier Sucking without tears with Thumb...   \n",
       "...                                                   ...   \n",
       "183526  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "183527  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "183528   Abstract 2 PK Baby / Toddler Training Cup (Pink)   \n",
       "183529  Baby Food Freezer Tray - Bacteria Resistant, B...   \n",
       "183530  Best 2 Pack Baby Car Shade for Kids - Window S...   \n",
       "\n",
       "                                                   review  rating  \\\n",
       "1       it came early and was not disappointed. i love...       5   \n",
       "2       Very soft and comfortable and warmer than it l...       5   \n",
       "3       This is a product well worth the purchase.  I ...       5   \n",
       "4       All of my kids have cried non-stop when I trie...       5   \n",
       "5       When the Binky Fairy came to our house, we did...       5   \n",
       "...                                                   ...     ...   \n",
       "183526  Such a great idea! very handy to have and look...       5   \n",
       "183527  This product rocks!  It is a great blend of fu...       5   \n",
       "183528  This item looks great and cool for my kids.......       5   \n",
       "183529  I am extremely happy with this product. I have...       5   \n",
       "183530  I love this product very mush . I have bought ...       5   \n",
       "\n",
       "                                             review_clean  \\\n",
       "1       it came early and was not disappointed i love ...   \n",
       "2       Very soft and comfortable and warmer than it l...   \n",
       "3       This is a product well worth the purchase  I h...   \n",
       "4       All of my kids have cried nonstop when I tried...   \n",
       "5       When the Binky Fairy came to our house we didn...   \n",
       "...                                                   ...   \n",
       "183526  Such a great idea very handy to have and look ...   \n",
       "183527  This product rocks  It is a great blend of fun...   \n",
       "183528  This item looks great and cool for my kidsI kn...   \n",
       "183529  I am extremely happy with this product I have ...   \n",
       "183530  I love this product very mush  I have bought m...   \n",
       "\n",
       "                                               word_count  sentiment  \n",
       "1       {'it': 3, 'came': 1, 'early': 1, 'and': 3, 'wa...          1  \n",
       "2       {'Very': 1, 'soft': 1, 'and': 2, 'comfortable'...          1  \n",
       "3       {'This': 1, 'is': 4, 'a': 2, 'product': 2, 'we...          1  \n",
       "4       {'All': 1, 'of': 1, 'my': 1, 'kids': 2, 'have'...          1  \n",
       "5       {'When': 1, 'the': 5, 'Binky': 3, 'Fairy': 3, ...          1  \n",
       "...                                                   ...        ...  \n",
       "183526  {'Such': 1, 'a': 1, 'great': 2, 'idea': 1, 've...          1  \n",
       "183527  {'This': 1, 'product': 2, 'rocks': 1, 'It': 1,...          1  \n",
       "183528  {'This': 1, 'item': 1, 'looks': 1, 'great': 2,...          1  \n",
       "183529  {'I': 9, 'am': 2, 'extremely': 1, 'happy': 1, ...          1  \n",
       "183530  {'I': 2, 'love': 1, 'this': 1, 'product': 1, '...          1  \n",
       "\n",
       "[166752 rows x 6 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['sentiment'] = products['rating'].apply(lambda rating : 1 if rating > 3 else -1)\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that the dataset contains an extra column called **sentiment** which is either positive (+1) or negative (-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, test_data = train_test_split(products, test_size = 0.2, stratify = products['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the train idx and test idx\n",
    "import json\n",
    "\n",
    "f = open('module-2-assignment-test-idx.json')\n",
    "test_idx = json.load(f)\n",
    "\n",
    "f = open('module-2-assignment-train-idx.json')\n",
    "train_idx = json.load(f)\n",
    "\n",
    "train_data = products.iloc[train_idx]\n",
    "test_data = products.iloc[test_idx]\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "\n",
    "train_matrix = vectorizer.fit_transform(train_data['review_clean'])\n",
    "# Second, convert the test data into a sparse matrix, using the same word-column mapping\n",
    "test_matrix = vectorizer.transform(test_data['review_clean'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a sentiment classifier with logistic regression\n",
    "\n",
    "We will now use logistic regression to create a sentiment classifier on the training data. This model will use the column **word_count** as a feature and the column **sentiment** as the target. We will use `validation_set=None` to obtain same results as everyone else.\n",
    "\n",
    "**Note:** This line may take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = turicreate.logistic_classifier.create(train_data,\n",
    "                                                        target = 'sentiment',\n",
    "                                                        features=['word_count'],\n",
    "                                                        validation_set=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I could not use turicreate so I will use sklearn instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = LogisticRegression().fit(X = train_matrix,\n",
    "                                           y = train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**. You may get a warning to the effect of \"Terminated due to numerical difficulties --- this model may not be ideal\". It means that the quality metric (to be covered in Module 3) failed to improve in the last iteration of the run. The difficulty arises as the sentiment model puts too much weight on extremely rare words. A way to rectify this is to apply regularization, to be covered in Module 4. Regularization lessens the effect of extremely rare words. For the purpose of this assignment, however, please proceed with the model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted the model, we can extract the weights (coefficients) as an SFrame as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121713"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = sentiment_model.coef_[0]\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of `121713` coefficients in the model. Recall from the lecture that positive weights $w_j$ correspond to weights that cause positive sentiment, while negative weights correspond to negative sentiment. \n",
    "\n",
    "Fill in the following block of code to calculate how many *weights* are positive ( >= 0). (**Hint**: The `'value'` column in SFrame *weights* must be positive ( >= 0))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive weights: 90262 \n",
      "Number of negative weights: 31451 \n"
     ]
    }
   ],
   "source": [
    "num_positive_weights = (weights >= 0).sum()\n",
    "num_negative_weights = len(weights) - num_positive_weights\n",
    "\n",
    "print(\"Number of positive weights: %s \" % num_positive_weights)\n",
    "print(\"Number of negative weights: %s \" % num_negative_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** How many weights are >= 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with logistic regression\n",
    "\n",
    "Now that a model is trained, we can make predictions on the **test data**. In this section, we will explore this in the context of 3 examples in the test dataset.  We refer to this set of 3 examples as the **sample_test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59    5\n",
      "71    2\n",
      "91    1\n",
      "Name: rating, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Our Baby Girl Memory Book</td>\n",
       "      <td>Absolutely love it and all of the Scripture in...</td>\n",
       "      <td>5</td>\n",
       "      <td>Absolutely love it and all of the Scripture in...</td>\n",
       "      <td>{'Absolutely': 1, 'love': 1, 'it': 2, 'and': 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Wall Decor Removable Decal Sticker - Colorful ...</td>\n",
       "      <td>Would not purchase again or recommend. The dec...</td>\n",
       "      <td>2</td>\n",
       "      <td>Would not purchase again or recommend The deca...</td>\n",
       "      <td>{'Would': 1, 'not': 1, 'purchase': 1, 'again':...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>New Style Trailing Cherry Blossom Tree Decal R...</td>\n",
       "      <td>Was so excited to get this product for my baby...</td>\n",
       "      <td>1</td>\n",
       "      <td>Was so excited to get this product for my baby...</td>\n",
       "      <td>{'Was': 1, 'so': 1, 'excited': 1, 'to': 3, 'ge...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  \\\n",
       "59                          Our Baby Girl Memory Book   \n",
       "71  Wall Decor Removable Decal Sticker - Colorful ...   \n",
       "91  New Style Trailing Cherry Blossom Tree Decal R...   \n",
       "\n",
       "                                               review  rating  \\\n",
       "59  Absolutely love it and all of the Scripture in...       5   \n",
       "71  Would not purchase again or recommend. The dec...       2   \n",
       "91  Was so excited to get this product for my baby...       1   \n",
       "\n",
       "                                         review_clean  \\\n",
       "59  Absolutely love it and all of the Scripture in...   \n",
       "71  Would not purchase again or recommend The deca...   \n",
       "91  Was so excited to get this product for my baby...   \n",
       "\n",
       "                                           word_count  sentiment  \n",
       "59  {'Absolutely': 1, 'love': 1, 'it': 2, 'and': 2...          1  \n",
       "71  {'Would': 1, 'not': 1, 'purchase': 1, 'again':...         -1  \n",
       "91  {'Was': 1, 'so': 1, 'excited': 1, 'to': 3, 'ge...         -1  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data = test_data[10:13]\n",
    "print(sample_test_data['rating'])\n",
    "sample_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Our Baby Girl Memory Book</td>\n",
       "      <td>Absolutely love it and all of the Scripture in...</td>\n",
       "      <td>5</td>\n",
       "      <td>Absolutely love it and all of the Scripture in...</td>\n",
       "      <td>{'Absolutely': 1, 'love': 1, 'it': 2, 'and': 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Wall Decor Removable Decal Sticker - Colorful ...</td>\n",
       "      <td>Would not purchase again or recommend. The dec...</td>\n",
       "      <td>2</td>\n",
       "      <td>Would not purchase again or recommend The deca...</td>\n",
       "      <td>{'Would': 1, 'not': 1, 'purchase': 1, 'again':...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>New Style Trailing Cherry Blossom Tree Decal R...</td>\n",
       "      <td>Was so excited to get this product for my baby...</td>\n",
       "      <td>1</td>\n",
       "      <td>Was so excited to get this product for my baby...</td>\n",
       "      <td>{'Was': 1, 'so': 1, 'excited': 1, 'to': 3, 'ge...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  \\\n",
       "59                          Our Baby Girl Memory Book   \n",
       "71  Wall Decor Removable Decal Sticker - Colorful ...   \n",
       "91  New Style Trailing Cherry Blossom Tree Decal R...   \n",
       "\n",
       "                                               review  rating  \\\n",
       "59  Absolutely love it and all of the Scripture in...       5   \n",
       "71  Would not purchase again or recommend. The dec...       2   \n",
       "91  Was so excited to get this product for my baby...       1   \n",
       "\n",
       "                                         review_clean  \\\n",
       "59  Absolutely love it and all of the Scripture in...   \n",
       "71  Would not purchase again or recommend The deca...   \n",
       "91  Was so excited to get this product for my baby...   \n",
       "\n",
       "                                           word_count  sentiment  \n",
       "59  {'Absolutely': 1, 'love': 1, 'it': 2, 'and': 2...          1  \n",
       "71  {'Would': 1, 'not': 1, 'purchase': 1, 'again':...         -1  \n",
       "91  {'Was': 1, 'so': 1, 'excited': 1, 'to': 3, 'ge...         -1  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[10:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Baby Tracker&amp;reg; - Daily Childcare Journal, S...</td>\n",
       "      <td>This book is perfect!  I'm a first time new mo...</td>\n",
       "      <td>5</td>\n",
       "      <td>This book is perfect  Im a first time new mom ...</td>\n",
       "      <td>{'This': 1, 'book': 2, 'is': 1, 'perfect': 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Baby Tracker&amp;reg; - Daily Childcare Journal, S...</td>\n",
       "      <td>I originally just gave the nanny a pad of pape...</td>\n",
       "      <td>4</td>\n",
       "      <td>I originally just gave the nanny a pad of pape...</td>\n",
       "      <td>{'I': 7, 'originally': 1, 'just': 1, 'gave': 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Nature's Lullabies First Year Sticker Calendar</td>\n",
       "      <td>Space for monthly photos, info and a lot of us...</td>\n",
       "      <td>5</td>\n",
       "      <td>Space for monthly photos info and a lot of use...</td>\n",
       "      <td>{'Space': 1, 'for': 1, 'monthly': 1, 'photos':...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  \\\n",
       "11  Baby Tracker&reg; - Daily Childcare Journal, S...   \n",
       "12  Baby Tracker&reg; - Daily Childcare Journal, S...   \n",
       "14     Nature's Lullabies First Year Sticker Calendar   \n",
       "\n",
       "                                               review  rating  \\\n",
       "11  This book is perfect!  I'm a first time new mo...       5   \n",
       "12  I originally just gave the nanny a pad of pape...       4   \n",
       "14  Space for monthly photos, info and a lot of us...       5   \n",
       "\n",
       "                                         review_clean  \\\n",
       "11  This book is perfect  Im a first time new mom ...   \n",
       "12  I originally just gave the nanny a pad of pape...   \n",
       "14  Space for monthly photos info and a lot of use...   \n",
       "\n",
       "                                           word_count  sentiment  \n",
       "11  {'This': 1, 'book': 2, 'is': 1, 'perfect': 1, ...          1  \n",
       "12  {'I': 7, 'originally': 1, 'just': 1, 'gave': 1...          1  \n",
       "14  {'Space': 1, 'for': 1, 'monthly': 1, 'photos':...          1  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.iloc[[10,11,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name            New Style Trailing Cherry Blossom Tree Decal R...\n",
       "review          Was so excited to get this product for my baby...\n",
       "rating                                                          1\n",
       "review_clean    Was so excited to get this product for my baby...\n",
       "word_count      {'Was': 1, 'so': 1, 'excited': 1, 'to': 3, 'ge...\n",
       "sentiment                                                      -1\n",
       "Name: 91, dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.iloc[82\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 9,\n",
       " 14,\n",
       " 18,\n",
       " 24,\n",
       " 31,\n",
       " 32,\n",
       " 36,\n",
       " 38,\n",
       " 50,\n",
       " 53,\n",
       " 64,\n",
       " 82,\n",
       " 102,\n",
       " 105,\n",
       " 106,\n",
       " 109,\n",
       " 112,\n",
       " 117,\n",
       " 123,\n",
       " 126,\n",
       " 132,\n",
       " 134,\n",
       " 142,\n",
       " 143,\n",
       " 145,\n",
       " 149,\n",
       " 156,\n",
       " 162,\n",
       " 165,\n",
       " 167,\n",
       " 170,\n",
       " 175,\n",
       " 176,\n",
       " 184,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 205,\n",
       " 217,\n",
       " 221,\n",
       " 223,\n",
       " 226,\n",
       " 229,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 239,\n",
       " 256,\n",
       " 258,\n",
       " 261,\n",
       " 263,\n",
       " 264,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 293,\n",
       " 326,\n",
       " 328,\n",
       " 335,\n",
       " 338,\n",
       " 350,\n",
       " 352,\n",
       " 356,\n",
       " 362,\n",
       " 366,\n",
       " 370,\n",
       " 382,\n",
       " 390,\n",
       " 398,\n",
       " 399,\n",
       " 401,\n",
       " 403,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 415,\n",
       " 418,\n",
       " 419,\n",
       " 425,\n",
       " 430,\n",
       " 431,\n",
       " 433,\n",
       " 434,\n",
       " 437,\n",
       " 438,\n",
       " 451,\n",
       " 453,\n",
       " 456,\n",
       " 464,\n",
       " 466,\n",
       " 469,\n",
       " 492,\n",
       " 504,\n",
       " 505,\n",
       " 509,\n",
       " 511,\n",
       " 519,\n",
       " 521,\n",
       " 526,\n",
       " 528,\n",
       " 534,\n",
       " 542,\n",
       " 543,\n",
       " 546,\n",
       " 551,\n",
       " 555,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 567,\n",
       " 574,\n",
       " 577,\n",
       " 580,\n",
       " 581,\n",
       " 585,\n",
       " 586,\n",
       " 591,\n",
       " 596,\n",
       " 600,\n",
       " 612,\n",
       " 614,\n",
       " 622,\n",
       " 623,\n",
       " 626,\n",
       " 630,\n",
       " 635,\n",
       " 637,\n",
       " 647,\n",
       " 651,\n",
       " 656,\n",
       " 657,\n",
       " 661,\n",
       " 662,\n",
       " 668,\n",
       " 671,\n",
       " 682,\n",
       " 690,\n",
       " 691,\n",
       " 696,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 718,\n",
       " 722,\n",
       " 733,\n",
       " 737,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 765,\n",
       " 779,\n",
       " 781,\n",
       " 784,\n",
       " 788,\n",
       " 790,\n",
       " 793,\n",
       " 802,\n",
       " 805,\n",
       " 817,\n",
       " 825,\n",
       " 827,\n",
       " 845,\n",
       " 848,\n",
       " 851,\n",
       " 853,\n",
       " 855,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 869,\n",
       " 871,\n",
       " 873,\n",
       " 884,\n",
       " 891,\n",
       " 893,\n",
       " 898,\n",
       " 899,\n",
       " 903,\n",
       " 907,\n",
       " 909,\n",
       " 912,\n",
       " 914,\n",
       " 920,\n",
       " 923,\n",
       " 924,\n",
       " 926,\n",
       " 929,\n",
       " 930,\n",
       " 933,\n",
       " 951,\n",
       " 962,\n",
       " 965,\n",
       " 967,\n",
       " 978,\n",
       " 982,\n",
       " 988,\n",
       " 995,\n",
       " 997,\n",
       " 999,\n",
       " 1009,\n",
       " 1010,\n",
       " 1014,\n",
       " 1018,\n",
       " 1023,\n",
       " 1035,\n",
       " 1043,\n",
       " 1047,\n",
       " 1055,\n",
       " 1056,\n",
       " 1057,\n",
       " 1061,\n",
       " 1064,\n",
       " 1068,\n",
       " 1070,\n",
       " 1074,\n",
       " 1077,\n",
       " 1078,\n",
       " 1081,\n",
       " 1083,\n",
       " 1085,\n",
       " 1093,\n",
       " 1097,\n",
       " 1102,\n",
       " 1105,\n",
       " 1116,\n",
       " 1118,\n",
       " 1122,\n",
       " 1124,\n",
       " 1129,\n",
       " 1130,\n",
       " 1131,\n",
       " 1136,\n",
       " 1143,\n",
       " 1147,\n",
       " 1153,\n",
       " 1154,\n",
       " 1156,\n",
       " 1158,\n",
       " 1163,\n",
       " 1166,\n",
       " 1177,\n",
       " 1185,\n",
       " 1193,\n",
       " 1195,\n",
       " 1199,\n",
       " 1200,\n",
       " 1203,\n",
       " 1211,\n",
       " 1217,\n",
       " 1241,\n",
       " 1242,\n",
       " 1244,\n",
       " 1252,\n",
       " 1254,\n",
       " 1257,\n",
       " 1259,\n",
       " 1262,\n",
       " 1263,\n",
       " 1265,\n",
       " 1266,\n",
       " 1276,\n",
       " 1284,\n",
       " 1287,\n",
       " 1290,\n",
       " 1297,\n",
       " 1300,\n",
       " 1310,\n",
       " 1311,\n",
       " 1315,\n",
       " 1322,\n",
       " 1324,\n",
       " 1325,\n",
       " 1328,\n",
       " 1330,\n",
       " 1335,\n",
       " 1336,\n",
       " 1344,\n",
       " 1346,\n",
       " 1351,\n",
       " 1354,\n",
       " 1358,\n",
       " 1359,\n",
       " 1361,\n",
       " 1362,\n",
       " 1367,\n",
       " 1369,\n",
       " 1386,\n",
       " 1391,\n",
       " 1395,\n",
       " 1396,\n",
       " 1399,\n",
       " 1400,\n",
       " 1402,\n",
       " 1417,\n",
       " 1420,\n",
       " 1428,\n",
       " 1429,\n",
       " 1431,\n",
       " 1432,\n",
       " 1436,\n",
       " 1439,\n",
       " 1444,\n",
       " 1452,\n",
       " 1464,\n",
       " 1481,\n",
       " 1482,\n",
       " 1483,\n",
       " 1487,\n",
       " 1492,\n",
       " 1499,\n",
       " 1510,\n",
       " 1515,\n",
       " 1530,\n",
       " 1538,\n",
       " 1554,\n",
       " 1562,\n",
       " 1580,\n",
       " 1589,\n",
       " 1594,\n",
       " 1600,\n",
       " 1601,\n",
       " 1603,\n",
       " 1610,\n",
       " 1611,\n",
       " 1615,\n",
       " 1617,\n",
       " 1621,\n",
       " 1623,\n",
       " 1630,\n",
       " 1631,\n",
       " 1635,\n",
       " 1636,\n",
       " 1640,\n",
       " 1644,\n",
       " 1648,\n",
       " 1649,\n",
       " 1650,\n",
       " 1651,\n",
       " 1659,\n",
       " 1667,\n",
       " 1668,\n",
       " 1684,\n",
       " 1700,\n",
       " 1703,\n",
       " 1706,\n",
       " 1708,\n",
       " 1709,\n",
       " 1714,\n",
       " 1724,\n",
       " 1733,\n",
       " 1734,\n",
       " 1735,\n",
       " 1741,\n",
       " 1748,\n",
       " 1753,\n",
       " 1755,\n",
       " 1761,\n",
       " 1766,\n",
       " 1767,\n",
       " 1770,\n",
       " 1771,\n",
       " 1773,\n",
       " 1805,\n",
       " 1806,\n",
       " 1807,\n",
       " 1809,\n",
       " 1814,\n",
       " 1818,\n",
       " 1824,\n",
       " 1830,\n",
       " 1836,\n",
       " 1848,\n",
       " 1857,\n",
       " 1860,\n",
       " 1870,\n",
       " 1881,\n",
       " 1884,\n",
       " 1889,\n",
       " 1892,\n",
       " 1911,\n",
       " 1912,\n",
       " 1918,\n",
       " 1920,\n",
       " 1924,\n",
       " 1926,\n",
       " 1933,\n",
       " 1935,\n",
       " 1936,\n",
       " 1946,\n",
       " 1947,\n",
       " 1951,\n",
       " 1959,\n",
       " 1963,\n",
       " 1966,\n",
       " 1972,\n",
       " 1981,\n",
       " 1984,\n",
       " 1989,\n",
       " 1991,\n",
       " 1993,\n",
       " 2001,\n",
       " 2002,\n",
       " 2003,\n",
       " 2010,\n",
       " 2012,\n",
       " 2013,\n",
       " 2024,\n",
       " 2028,\n",
       " 2044,\n",
       " 2045,\n",
       " 2047,\n",
       " 2056,\n",
       " 2060,\n",
       " 2065,\n",
       " 2067,\n",
       " 2081,\n",
       " 2085,\n",
       " 2088,\n",
       " 2093,\n",
       " 2103,\n",
       " 2105,\n",
       " 2109,\n",
       " 2111,\n",
       " 2113,\n",
       " 2119,\n",
       " 2127,\n",
       " 2129,\n",
       " 2130,\n",
       " 2133,\n",
       " 2134,\n",
       " 2136,\n",
       " 2145,\n",
       " 2146,\n",
       " 2149,\n",
       " 2152,\n",
       " 2155,\n",
       " 2156,\n",
       " 2159,\n",
       " 2161,\n",
       " 2167,\n",
       " 2176,\n",
       " 2183,\n",
       " 2186,\n",
       " 2187,\n",
       " 2196,\n",
       " 2202,\n",
       " 2209,\n",
       " 2233,\n",
       " 2239,\n",
       " 2243,\n",
       " 2247,\n",
       " 2248,\n",
       " 2251,\n",
       " 2252,\n",
       " 2256,\n",
       " 2257,\n",
       " 2260,\n",
       " 2268,\n",
       " 2280,\n",
       " 2283,\n",
       " 2284,\n",
       " 2288,\n",
       " 2294,\n",
       " 2296,\n",
       " 2305,\n",
       " 2311,\n",
       " 2318,\n",
       " 2319,\n",
       " 2321,\n",
       " 2322,\n",
       " 2324,\n",
       " 2333,\n",
       " 2336,\n",
       " 2337,\n",
       " 2339,\n",
       " 2341,\n",
       " 2345,\n",
       " 2346,\n",
       " 2350,\n",
       " 2355,\n",
       " 2356,\n",
       " 2367,\n",
       " 2374,\n",
       " 2377,\n",
       " 2389,\n",
       " 2394,\n",
       " 2396,\n",
       " 2401,\n",
       " 2416,\n",
       " 2417,\n",
       " 2419,\n",
       " 2420,\n",
       " 2426,\n",
       " 2432,\n",
       " 2436,\n",
       " 2437,\n",
       " 2439,\n",
       " 2451,\n",
       " 2455,\n",
       " 2459,\n",
       " 2460,\n",
       " 2464,\n",
       " 2471,\n",
       " 2479,\n",
       " 2483,\n",
       " 2485,\n",
       " 2494,\n",
       " 2496,\n",
       " 2499,\n",
       " 2500,\n",
       " 2504,\n",
       " 2519,\n",
       " 2522,\n",
       " 2525,\n",
       " 2532,\n",
       " 2540,\n",
       " 2549,\n",
       " 2559,\n",
       " 2562,\n",
       " 2563,\n",
       " 2571,\n",
       " 2574,\n",
       " 2581,\n",
       " 2585,\n",
       " 2588,\n",
       " 2592,\n",
       " 2600,\n",
       " 2609,\n",
       " 2617,\n",
       " 2624,\n",
       " 2628,\n",
       " 2634,\n",
       " 2636,\n",
       " 2640,\n",
       " 2653,\n",
       " 2662,\n",
       " 2669,\n",
       " 2670,\n",
       " 2675,\n",
       " 2677,\n",
       " 2681,\n",
       " 2684,\n",
       " 2686,\n",
       " 2698,\n",
       " 2701,\n",
       " 2709,\n",
       " 2710,\n",
       " 2714,\n",
       " 2715,\n",
       " 2718,\n",
       " 2719,\n",
       " 2722,\n",
       " 2724,\n",
       " 2726,\n",
       " 2728,\n",
       " 2730,\n",
       " 2740,\n",
       " 2741,\n",
       " 2742,\n",
       " 2745,\n",
       " 2747,\n",
       " 2754,\n",
       " 2766,\n",
       " 2767,\n",
       " 2772,\n",
       " 2776,\n",
       " 2778,\n",
       " 2781,\n",
       " 2795,\n",
       " 2811,\n",
       " 2815,\n",
       " 2816,\n",
       " 2817,\n",
       " 2824,\n",
       " 2825,\n",
       " 2827,\n",
       " 2830,\n",
       " 2831,\n",
       " 2836,\n",
       " 2837,\n",
       " 2839,\n",
       " 2840,\n",
       " 2841,\n",
       " 2852,\n",
       " 2854,\n",
       " 2855,\n",
       " 2858,\n",
       " 2862,\n",
       " 2866,\n",
       " 2874,\n",
       " 2889,\n",
       " 2893,\n",
       " 2894,\n",
       " 2899,\n",
       " 2900,\n",
       " 2909,\n",
       " 2921,\n",
       " 2926,\n",
       " 2935,\n",
       " 2939,\n",
       " 2940,\n",
       " 2943,\n",
       " 2947,\n",
       " 2948,\n",
       " 2962,\n",
       " 2963,\n",
       " 2965,\n",
       " 2966,\n",
       " 2973,\n",
       " 2976,\n",
       " 2987,\n",
       " 3005,\n",
       " 3008,\n",
       " 3021,\n",
       " 3031,\n",
       " 3045,\n",
       " 3048,\n",
       " 3054,\n",
       " 3059,\n",
       " 3060,\n",
       " 3069,\n",
       " 3076,\n",
       " 3087,\n",
       " 3088,\n",
       " 3089,\n",
       " 3095,\n",
       " 3100,\n",
       " 3102,\n",
       " 3112,\n",
       " 3114,\n",
       " 3115,\n",
       " 3117,\n",
       " 3139,\n",
       " 3140,\n",
       " 3143,\n",
       " 3147,\n",
       " 3150,\n",
       " 3158,\n",
       " 3159,\n",
       " 3166,\n",
       " 3173,\n",
       " 3178,\n",
       " 3195,\n",
       " 3196,\n",
       " 3200,\n",
       " 3218,\n",
       " 3221,\n",
       " 3222,\n",
       " 3235,\n",
       " 3242,\n",
       " 3247,\n",
       " 3249,\n",
       " 3253,\n",
       " 3254,\n",
       " 3255,\n",
       " 3263,\n",
       " 3265,\n",
       " 3267,\n",
       " 3279,\n",
       " 3284,\n",
       " 3299,\n",
       " 3304,\n",
       " 3306,\n",
       " 3308,\n",
       " 3313,\n",
       " 3320,\n",
       " 3321,\n",
       " 3324,\n",
       " 3332,\n",
       " 3337,\n",
       " 3345,\n",
       " 3347,\n",
       " 3351,\n",
       " 3359,\n",
       " 3361,\n",
       " 3364,\n",
       " 3366,\n",
       " 3367,\n",
       " 3376,\n",
       " 3378,\n",
       " 3382,\n",
       " 3387,\n",
       " 3393,\n",
       " 3394,\n",
       " 3398,\n",
       " 3404,\n",
       " 3411,\n",
       " 3412,\n",
       " 3413,\n",
       " 3414,\n",
       " 3416,\n",
       " 3422,\n",
       " 3423,\n",
       " 3426,\n",
       " 3429,\n",
       " 3431,\n",
       " 3435,\n",
       " 3438,\n",
       " 3440,\n",
       " 3443,\n",
       " 3450,\n",
       " 3457,\n",
       " 3465,\n",
       " 3470,\n",
       " 3471,\n",
       " 3476,\n",
       " 3479,\n",
       " 3485,\n",
       " 3487,\n",
       " 3490,\n",
       " 3494,\n",
       " 3497,\n",
       " 3502,\n",
       " 3535,\n",
       " 3538,\n",
       " 3549,\n",
       " 3554,\n",
       " 3560,\n",
       " 3569,\n",
       " 3583,\n",
       " 3587,\n",
       " 3596,\n",
       " 3609,\n",
       " 3612,\n",
       " 3613,\n",
       " 3618,\n",
       " 3631,\n",
       " 3656,\n",
       " 3660,\n",
       " 3664,\n",
       " 3665,\n",
       " 3680,\n",
       " 3682,\n",
       " 3694,\n",
       " 3698,\n",
       " 3707,\n",
       " 3721,\n",
       " 3723,\n",
       " 3726,\n",
       " 3734,\n",
       " 3741,\n",
       " 3747,\n",
       " 3748,\n",
       " 3751,\n",
       " 3756,\n",
       " 3758,\n",
       " 3780,\n",
       " 3782,\n",
       " 3784,\n",
       " 3788,\n",
       " 3789,\n",
       " 3798,\n",
       " 3806,\n",
       " 3807,\n",
       " 3810,\n",
       " 3815,\n",
       " 3817,\n",
       " 3823,\n",
       " 3825,\n",
       " 3829,\n",
       " 3830,\n",
       " 3832,\n",
       " 3836,\n",
       " 3837,\n",
       " 3840,\n",
       " 3844,\n",
       " 3850,\n",
       " 3851,\n",
       " 3854,\n",
       " 3860,\n",
       " 3861,\n",
       " 3864,\n",
       " 3866,\n",
       " 3867,\n",
       " 3872,\n",
       " 3884,\n",
       " 3885,\n",
       " 3886,\n",
       " 3893,\n",
       " 3902,\n",
       " 3903,\n",
       " 3907,\n",
       " 3909,\n",
       " 3911,\n",
       " 3914,\n",
       " 3915,\n",
       " 3926,\n",
       " 3927,\n",
       " 3930,\n",
       " 3933,\n",
       " 3934,\n",
       " 3936,\n",
       " 3940,\n",
       " 3942,\n",
       " 3944,\n",
       " 3945,\n",
       " 3946,\n",
       " 3947,\n",
       " 3949,\n",
       " 3953,\n",
       " 3955,\n",
       " 3958,\n",
       " 3959,\n",
       " 3962,\n",
       " 3963,\n",
       " 3966,\n",
       " 3967,\n",
       " 3970,\n",
       " 3972,\n",
       " 3973,\n",
       " 3975,\n",
       " 3979,\n",
       " 3981,\n",
       " 3982,\n",
       " 3983,\n",
       " 3984,\n",
       " 3998,\n",
       " 3999,\n",
       " 4003,\n",
       " 4005,\n",
       " 4006,\n",
       " 4007,\n",
       " 4012,\n",
       " 4013,\n",
       " 4020,\n",
       " 4027,\n",
       " 4033,\n",
       " 4034,\n",
       " 4037,\n",
       " 4043,\n",
       " 4046,\n",
       " 4060,\n",
       " 4064,\n",
       " 4065,\n",
       " 4066,\n",
       " 4068,\n",
       " 4075,\n",
       " 4080,\n",
       " 4082,\n",
       " 4091,\n",
       " 4093,\n",
       " 4100,\n",
       " 4106,\n",
       " 4114,\n",
       " 4118,\n",
       " 4121,\n",
       " 4122,\n",
       " 4137,\n",
       " 4141,\n",
       " 4144,\n",
       " 4146,\n",
       " 4152,\n",
       " 4161,\n",
       " 4170,\n",
       " 4171,\n",
       " 4179,\n",
       " 4181,\n",
       " 4186,\n",
       " 4192,\n",
       " 4200,\n",
       " 4210,\n",
       " 4213,\n",
       " 4220,\n",
       " 4227,\n",
       " 4231,\n",
       " 4233,\n",
       " 4237,\n",
       " 4245,\n",
       " 4254,\n",
       " 4258,\n",
       " 4264,\n",
       " 4275,\n",
       " 4276,\n",
       " 4278,\n",
       " 4283,\n",
       " 4285,\n",
       " 4294,\n",
       " 4298,\n",
       " 4313,\n",
       " 4316,\n",
       " 4341,\n",
       " 4342,\n",
       " 4344,\n",
       " 4346,\n",
       " 4351,\n",
       " 4354,\n",
       " 4359,\n",
       " 4361,\n",
       " 4363,\n",
       " 4365,\n",
       " 4373,\n",
       " 4376,\n",
       " 4382,\n",
       " 4383,\n",
       " 4384,\n",
       " 4387,\n",
       " 4389,\n",
       " 4397,\n",
       " 4403,\n",
       " 4417,\n",
       " 4420,\n",
       " 4425,\n",
       " 4429,\n",
       " 4436,\n",
       " 4442,\n",
       " 4449,\n",
       " 4450,\n",
       " 4468,\n",
       " 4472,\n",
       " 4475,\n",
       " 4480,\n",
       " 4487,\n",
       " 4494,\n",
       " 4495,\n",
       " 4498,\n",
       " 4501,\n",
       " 4503,\n",
       " 4507,\n",
       " 4508,\n",
       " 4509,\n",
       " 4510,\n",
       " 4512,\n",
       " 4514,\n",
       " 4517,\n",
       " 4518,\n",
       " 4525,\n",
       " 4526,\n",
       " 4529,\n",
       " 4535,\n",
       " 4543,\n",
       " 4544,\n",
       " 4549,\n",
       " 4554,\n",
       " 4565,\n",
       " 4567,\n",
       " 4569,\n",
       " 4578,\n",
       " 4579,\n",
       " 4584,\n",
       " 4589,\n",
       " 4592,\n",
       " 4594,\n",
       " 4595,\n",
       " 4601,\n",
       " 4602,\n",
       " 4611,\n",
       " 4614,\n",
       " 4617,\n",
       " 4621,\n",
       " 4624,\n",
       " 4625,\n",
       " 4626,\n",
       " 4636,\n",
       " 4641,\n",
       " 4644,\n",
       " 4650,\n",
       " 4658,\n",
       " 4669,\n",
       " 4685,\n",
       " 4701,\n",
       " 4702,\n",
       " 4703,\n",
       " 4718,\n",
       " 4720,\n",
       " 4727,\n",
       " 4729,\n",
       " 4738,\n",
       " 4739,\n",
       " 4740,\n",
       " 4742,\n",
       " 4743,\n",
       " 4747,\n",
       " 4751,\n",
       " 4760,\n",
       " 4763,\n",
       " 4764,\n",
       " 4768,\n",
       " 4770,\n",
       " 4774,\n",
       " 4778,\n",
       " 4782,\n",
       " 4785,\n",
       " 4792,\n",
       " 4796,\n",
       " 4797,\n",
       " 4801,\n",
       " 4802,\n",
       " 4803,\n",
       " 4807,\n",
       " 4808,\n",
       " 4813,\n",
       " 4818,\n",
       " 4820,\n",
       " 4833,\n",
       " ...]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig deeper into the first row of the **sample_test_data**. Here's the full review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Absolutely love it and all of the Scripture in it.  I purchased the Baby Boy version for my grandson when he was born and my daughter-in-law was thrilled to receive the same book again.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data.iloc[0]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That review seems pretty positive.\n",
    "\n",
    "Now, let's see what the next row of the **sample_test_data** looks like. As we could guess from the sentiment (-1), the review is quite negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Would not purchase again or recommend. The decals were thick almost plastic like and were coming off the wall as I was applying them! The would NOT stick! Literally stayed stuck for about 5 minutes then started peeling off.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data.iloc[1]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make a **class** prediction for the **sample_test_data**. The `sentiment_model` should predict **+1** if the sentiment is positive and **-1** if the sentiment is negative. Recall from the lecture that the **score** (sometimes called **margin**) for the logistic regression model  is defined as:\n",
    "\n",
    "$$\n",
    "\\mbox{score}_i = \\mathbf{w}^T h(\\mathbf{x}_i)\n",
    "$$ \n",
    "\n",
    "where $h(\\mathbf{x}_i)$ represents the features for example $i$.  We will write some code to obtain the **scores** using Turi Create. For each row, the **score** (or margin) is a number in the range **[-inf, inf]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.62289151e-03 9.94377108e-01]\n",
      " [9.60597475e-01 3.94025251e-02]\n",
      " [9.99984400e-01 1.56000595e-05]]\n"
     ]
    }
   ],
   "source": [
    "scores = sentiment_model.predict_proba(vectorizer.transform(sample_test_data['review_clean']).toarray())\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Our Baby Girl Memory Book</td>\n",
       "      <td>Absolutely love it and all of the Scripture in...</td>\n",
       "      <td>5</td>\n",
       "      <td>Absolutely love it and all of the Scripture in...</td>\n",
       "      <td>{'Absolutely': 1, 'love': 1, 'it': 2, 'and': 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Wall Decor Removable Decal Sticker - Colorful ...</td>\n",
       "      <td>Would not purchase again or recommend. The dec...</td>\n",
       "      <td>2</td>\n",
       "      <td>Would not purchase again or recommend The deca...</td>\n",
       "      <td>{'Would': 1, 'not': 1, 'purchase': 1, 'again':...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>New Style Trailing Cherry Blossom Tree Decal R...</td>\n",
       "      <td>Was so excited to get this product for my baby...</td>\n",
       "      <td>1</td>\n",
       "      <td>Was so excited to get this product for my baby...</td>\n",
       "      <td>{'Was': 1, 'so': 1, 'excited': 1, 'to': 3, 'ge...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  \\\n",
       "59                          Our Baby Girl Memory Book   \n",
       "71  Wall Decor Removable Decal Sticker - Colorful ...   \n",
       "91  New Style Trailing Cherry Blossom Tree Decal R...   \n",
       "\n",
       "                                               review  rating  \\\n",
       "59  Absolutely love it and all of the Scripture in...       5   \n",
       "71  Would not purchase again or recommend. The dec...       2   \n",
       "91  Was so excited to get this product for my baby...       1   \n",
       "\n",
       "                                         review_clean  \\\n",
       "59  Absolutely love it and all of the Scripture in...   \n",
       "71  Would not purchase again or recommend The deca...   \n",
       "91  Was so excited to get this product for my baby...   \n",
       "\n",
       "                                           word_count  sentiment  \n",
       "59  {'Absolutely': 1, 'love': 1, 'it': 2, 'and': 2...          1  \n",
       "71  {'Would': 1, 'not': 1, 'purchase': 1, 'again':...         -1  \n",
       "91  {'Was': 1, 'so': 1, 'excited': 1, 'to': 3, 'ge...         -1  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting sentiment\n",
    "\n",
    "These scores can be used to make class predictions as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{w}^T h(\\mathbf{x}_i) > 0 \\\\\n",
    "      -1 & \\mathbf{w}^T h(\\mathbf{x}_i) \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Using scores, write code to calculate $\\hat{y}$, the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 121713)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(sample_test_data['review_clean']).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121713,)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, -1, -1]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(weight, x):\n",
    "    score = np.dot(x, weight)\n",
    "    return list(map(lambda x: 1 if x > 0 else -1, score))\n",
    "\n",
    "predict(weights, vectorizer.transform(sample_test_data['review_clean']).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from Turi Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions according to SKlearn:\n",
      "[ 1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Class predictions according to SKlearn:\")\n",
    "print(sentiment_model.predict(vectorizer.transform(sample_test_data['review_clean']).toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: Make sure your class predictions match with the one obtained from Turi Create.\n",
    "\n",
    "### Probability predictions\n",
    "\n",
    "Recall from the lectures that we can also calculate the probability predictions from the scores using:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))}.\n",
    "$$\n",
    "\n",
    "Using the variable **scores** calculated previously, write code to calculate the probability that a sentiment is positive using the above formula. For each row, the probabilities should be a number in the range **[0, 1]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(w, x):\n",
    "    score = np.dot(x, w)\n",
    "    prob = 1/(1+np.exp(-score))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.80163749e-01, 1.13313331e-02, 4.35891860e-06])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_prob(weights, vectorizer.transform(sample_test_data['review_clean']).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: Make sure your probability predictions match the ones obtained from Turi Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions according to Turi Create:\n",
      "[[5.62289151e-03 9.94377108e-01]\n",
      " [9.60597475e-01 3.94025251e-02]\n",
      " [9.99984400e-01 1.56000595e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Class predictions according to Turi Create:\")\n",
    "print(sentiment_model.predict_proba(vectorizer.transform(sample_test_data['review_clean']).toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Of the three data points in **sample_test_data**, which one (first, second, or third) has the **lowest probability** of being classified as a positive review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the most positive (and negative) review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to examining the full test dataset, **test_data**, and use Turi Create to form predictions on all of the test data points for faster performance.\n",
    "\n",
    "Using the `sentiment_model`, find the 20 reviews in the entire **test_data** with the **highest probability** of being classified as a **positive review**. We refer to these as the \"most positive reviews.\"\n",
    "\n",
    "To calculate these top-20 reviews, use the following steps:\n",
    "1.  Make probability predictions on **test_data** using the `sentiment_model`. (**Hint:** When you call `.predict` to make predictions on the test data, use option `output_type='probability'` to output the probability rather than just the most likely class.)\n",
    "2.  Sort the data according to those predictions and pick the top 20. (**Hint:** You can use the `.topk` method on an SFrame to find the top k rows sorted according to the value of a specified column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BABYBJORN Potty Chair - Red',\n",
       " 'Baby Einstein Around The World Discovery Center',\n",
       " 'Baby Jogger City Mini GT Double Stroller, Shadow/Orange',\n",
       " 'Baby Jogger City Mini GT Single Stroller, Shadow/Orange',\n",
       " 'Britax 2012 B-Agile Stroller, Red',\n",
       " 'Britax Boulevard 70-G3 Convertible Car Seat Seat, Onyx',\n",
       " 'Britax Decathlon Convertible Car Seat, Tiffany',\n",
       " 'Britax Frontier Booster Car Seat',\n",
       " 'Buttons Cloth Diaper Cover - One Size - 8 Color Options',\n",
       " 'Delta Universal 6 Drawer Dresser, Black Cherry',\n",
       " 'Diono RadianRXT Convertible Car Seat, Plum',\n",
       " 'EZ Squeezees Refillable Food Pouches,sold in pack of 3. 3 pouches each',\n",
       " 'Emily Green 6&quot; Bowl, Sunshine Safari',\n",
       " 'Evenflo 6 Pack Classic Glass Bottle, 4-Ounce',\n",
       " 'Evenflo X Sport Plus Convenience Stroller - Christina',\n",
       " \"Fisher-Price Cradle 'N Swing,  My Little Snugabunny\",\n",
       " 'Fisher-Price Zen Collection Cradle Swing',\n",
       " 'Freemie Hands-Free Concealable Breast Pump Collection System',\n",
       " 'Graco FastAction Fold Jogger Click Connect Stroller, Grapeade',\n",
       " \"Graco Pack 'n Play Element Playard - Flint\",\n",
       " 'Graco Quattro Tour Duo Stroller, Clairmont',\n",
       " 'Ikea 36 Pcs Kalas Kids Plastic BPA Free Flatware, Bowl, Plate, Tumbler Set, Colorful',\n",
       " 'Infantino Wrap and Tie Baby Carrier, Black Blueberries',\n",
       " 'Joovy Groove Ultralight Umbrella Stroller, Charcoal',\n",
       " 'Mamas &amp; Papas 2014 Urbo2 Stroller - Black',\n",
       " 'Munchkin Mozart Magic Cube',\n",
       " \"P'Kolino Silly Soft Seating in Tias, Green\",\n",
       " 'Phil and Teds Explorer Stroller With Doubles Kit in Black',\n",
       " 'Rainy Day Indoor Playground toddler swing to be used with support system',\n",
       " 'Roan Rocco Classic Pram Stroller 2-in-1 with Bassinet and Seat Unit - Coffee',\n",
       " 'Simple Wishes Hands-Free Breastpump Bra, Pink, XS-L',\n",
       " 'Skip Hop Bento Diaper Tote Bag, Black',\n",
       " 'Skip Hop Studio Diaper Bag, Black Dot',\n",
       " 'Stokke Scoot Stroller - Light Green',\n",
       " 'Stork Craft Beatrice Combo Tower Chest, White',\n",
       " 'Summer Infant 3-Stage Superseat Highchair, Green',\n",
       " 'Summer Infant Wide View Digital Color Video Monitor',\n",
       " 'The First Years Compass Pathway B570 Adjustable Booster Seat, Black and Khaki',\n",
       " 'The First Years True Fit Convertible Car Seat, Monet',\n",
       " 'We Sell Mats 36 Sq Ft Alphabet and Number Floor Mat'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20 = sentiment_model.predict_proba(test_matrix)[:,1].argsort()[-40:][::-1]\n",
    "set(test_data.iloc[top20]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Which of the following products are represented in the 20 most positive reviews? [multiple choice]\n",
    "\n",
    "\n",
    "Now, let us repeat this exercise to find the \"most negative reviews.\" Use the prediction probabilities to find the  20 reviews in the **test_data** with the **lowest probability** of being classified as a **positive review**. Repeat the same steps above but make sure you **sort in the opposite order**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94560     The First Years True Choice P400 Premium Digit...\n",
       "16042           Fisher-Price Ocean Wonders Aquarium Bouncer\n",
       "120209    Levana Safe N'See Digital Video Baby Monitor w...\n",
       "155287    VTech Communications Safe &amp; Sounds Full Co...\n",
       "53207                   Safety 1st High-Def Digital Monitor\n",
       "167249    Samsung SEW-3037W Wireless Pan Tilt Video Baby...\n",
       "81332                 Cloth Diaper Sprayer--styles may vary\n",
       "48694     Adiri BPA Free Natural Nurser Ultimate Bottle ...\n",
       "176046         Baby Trend Inertia Infant Car Seat - Horizon\n",
       "59546                Ellaroo Mei Tai Baby Carrier - Hershey\n",
       "95420          One Step Ahead Hide-Away Extra Long Bed Rail\n",
       "77072        Safety 1st Exchangeable Tip 3 in 1 Thermometer\n",
       "96572      Baby Jogger Summit XC Double Stroller, Red/Black\n",
       "94389                  Snuza Portable Baby Movement Monitor\n",
       "76000            Peg-Perego Tatamia High Chair, White Latte\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_ = sentiment_model.predict_proba(test_matrix)[:,1].argsort()[:15]\n",
    "test_data.iloc[top20_]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.49867426e-15, 2.21259188e-15, 2.13904525e-13, 1.19073927e-12,\n",
       "       6.27557995e-11, 1.14304815e-10, 1.31534025e-10, 1.57376501e-10,\n",
       "       1.74132430e-10, 1.83164621e-10, 2.10702361e-10, 2.15079418e-10,\n",
       "       2.48821777e-10, 3.48448997e-10, 4.01128228e-10])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.predict_proba(test_matrix)[:,1][top20_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Quiz Question**: Which of the following products are represented in the 20 most negative reviews?  [multiple choice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy of the classifier\n",
    "\n",
    "We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified examples}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "This can be computed as follows:\n",
    "\n",
    "* **Step 1:** Use the trained model to compute class predictions (**Hint:** Use the `predict` method)\n",
    "* **Step 2:** Count the number of data points when the predicted class labels match the ground truth labels (called `true_labels` below).\n",
    "* **Step 3:** Divide the total number of correct predictions by the total number of data points in the dataset.\n",
    "\n",
    "Complete the function below to compute the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    # First get the predictions\n",
    "    ## YOUR CODE HERE\n",
    "    pred = model.predict(data)\n",
    "    \n",
    "    # Compute the number of correctly classified examples\n",
    "    ## YOUR CODE HERE\n",
    "    n_correct = (pred == true_labels).sum()\n",
    "    \n",
    "    # Then compute accuracy by dividing num_correct by total number of examples\n",
    "    ## YOUR CODE HERE\n",
    "    accuracy = n_correct/len(true_labels)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the classification accuracy of the **sentiment_model** on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9319654427645788\n",
      "0.9478698207111591\n"
     ]
    }
   ],
   "source": [
    "print(get_classification_accuracy(sentiment_model, test_matrix, test_data['sentiment']))\n",
    "print(get_classification_accuracy(sentiment_model, train_matrix, train_data['sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: What is the accuracy of the **sentiment_model** on the **test_data**? Round your answer to 2 decimal places (e.g. 0.76).\n",
    "\n",
    "**Quiz Question**: Does a higher accuracy value on the **training_data** always imply that the classifier is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn another classifier with fewer words\n",
    "\n",
    "There were a lot of words in the model we trained above. We will now train a simpler logistic regression model using only a subset of words that occur in the reviews. For this assignment, we selected a 20 words to work with. These are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(significant_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each review, we will use the **word_count** column and trim out all words that are **not** in the **significant_words** list above. We will use the [SArray dictionary trim by keys functionality]( https://dato.com/products/create/docs/generated/graphlab.SArray.dict_trim_by_keys.html). Note that we are performing this on both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_trim_by_keys(mydict, List):\n",
    "    tmp = {}\n",
    "    \n",
    "    for key in mydict.keys():\n",
    "        if key in List:\n",
    "            tmp[key] = mydict[key]\n",
    "    \n",
    "    return tmp\n",
    "    \n",
    "\n",
    "train_data['word_count_subset'] = train_data['word_count'].apply(lambda x: dict_trim_by_keys(x, significant_words))\n",
    "test_data['word_count_subset'] = test_data['word_count'].apply(lambda x: dict_trim_by_keys(x, significant_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the first example of the dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it came early and was not disappointed. i love planet wise bags and now my wipe holder. it keps my osocozy wipes moist and does not leak. highly recommend it.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[0]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **word_count** column had been working with before looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 3, 'came': 1, 'early': 1, 'and': 3, 'was': 1, 'not': 2, 'disappointed': 1, 'i': 1, 'love': 1, 'planet': 1, 'wise': 1, 'bags': 1, 'now': 1, 'my': 2, 'wipe': 1, 'holder': 1, 'keps': 1, 'osocozy': 1, 'wipes': 1, 'moist': 1, 'does': 1, 'leak': 1, 'highly': 1, 'recommend': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.iloc[0]['word_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only working with a subset of these words, the column **word_count_subset** is a subset of the above dictionary. In this example, only 2 `significant words` are present in this review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disappointed': 1, 'love': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.iloc[0]['word_count_subset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a logistic regression model on a subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a classifier with **word_count_subset** as the feature and **sentiment** as the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = turicreate.logistic_classifier.create(train_data,\n",
    "                                                     target = 'sentiment',\n",
    "                                                     features=['word_count_subset'],\n",
    "                                                     validation_set=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(text, significant_words):\n",
    "    return ' '.join([x for x in text.split() if x in significant_words])\n",
    "    \n",
    "simple_train_data = train_data['review_clean'].apply(lambda x: simplify(x,significant_words))\n",
    "simple_test_data = test_data['review_clean'].apply(lambda x: simplify(x,significant_words))\n",
    "\n",
    "simple_vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "\n",
    "simple_train_matrix = vectorizer.fit_transform(simple_train_data)\n",
    "simple_test_matrix = vectorizer.transform(simple_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = LogisticRegression().fit(X = simple_train_matrix,\n",
    "                                           y = train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the classification accuracy using the `get_classification_accuracy` function you implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8673506119510439"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_classification_accuracy(simple_model, test_data, test_data['sentiment'])\n",
    "get_classification_accuracy(simple_model, simple_test_matrix, test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will inspect the weights (coefficients) of the **simple_model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.18634918, -1.64844682,  0.07445436, -2.31404997,  1.13217528,\n",
       "       -0.51084026,  0.85266174, -0.21736871,  0.54211338,  1.29752527,\n",
       "        1.67419808, -0.933143  ,  0.09912872,  1.44165452, -0.29823398,\n",
       "       -2.08905855, -1.91959733,  0.55693256, -0.63570201, -0.38790139])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the coefficients (in descending order) by the **value** to obtain the coefficients with the most positive effect on the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['able', 'broke', 'car', 'disappointed', 'easy', 'even', 'great',\n",
       "       'less', 'little', 'love', 'loves', 'money', 'old', 'perfect',\n",
       "       'product', 'return', 'waste', 'well', 'work', 'would'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(simple_train_data)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1.6741980788170672, 'loves'},\n",
       " {1.4416545217609096, 'perfect'},\n",
       " {1.2975252689896872, 'love'},\n",
       " {1.1321752792601463, 'easy'},\n",
       " {0.8526617429803622, 'great'},\n",
       " {0.5569325573662659, 'well'},\n",
       " {0.5421133808158118, 'little'},\n",
       " {0.18634918424700583, 'able'},\n",
       " {0.09912871688257827, 'old'},\n",
       " {0.07445435660225573, 'car'},\n",
       " {-0.21736870809605957, 'less'},\n",
       " {-0.29823397973218413, 'product'},\n",
       " {-0.38790138825475795, 'would'},\n",
       " {-0.5108402550690643, 'even'},\n",
       " {-0.6357020105459842, 'work'},\n",
       " {-0.9331429961616524, 'money'},\n",
       " {-1.6484468158007455, 'broke'},\n",
       " {-1.919597326961822, 'waste'},\n",
       " {-2.0890585546577625, 'return'},\n",
       " {-2.314049973706252, 'disappointed'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simple_model.coefficients.sort('value', ascending=False).print_rows(num_rows=21)\n",
    "coef_idx = simple_model.coef_[0].argsort()[-20:][::-1]\n",
    "feature_names = vectorizer.get_feature_names_out()[coef_idx]\n",
    "feature_coef = [{key, value} for key, value in zip(feature_names, simple_model.coef_[0][coef_idx])]\n",
    "\n",
    "feature_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Consider the coefficients of **simple_model**. There should be 21 of them, an intercept term + one for each word in **significant_words**. How many of the 20 coefficients (corresponding to the 20 **significant_words** and *excluding the intercept term*) are positive for the `simple_model`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(simple_model.coef_[0] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Are the positive words in the **simple_model** (let us call them `positive_significant_words`) also positive words in the **sentiment_model**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words =  feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words_idx = list(map(lambda x: list(vectorizer.get_feature_names_out()).index(x), positive_words))\n",
    "positive_words_idx_coef_ = sentiment_model.coef_[0][positive_words_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.77226011,  2.02081328,  1.60505608,  1.38917734,  1.27450867,\n",
       "        0.48203093,  0.54505152,  0.32870157, -0.02069284,  0.11950963])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words_idx_coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the accuracy of the **sentiment_model** and the **simple_model** using the `get_classification_accuracy` method you implemented above.\n",
    "\n",
    "First, compute the classification accuracy of the **sentiment_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9478698207111591"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(sentiment_model, train_matrix, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the classification accuracy of the **simple_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8644540384961324"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(simple_model, simple_train_matrix, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TRAINING set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will repeat this exercise on the **test_data**. Start by computing the classification accuracy of the **sentiment_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9319654427645788"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(sentiment_model, test_matrix, test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the classification accuracy of the **simple_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8673506119510439"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(simple_model, simple_test_matrix, test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TEST set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "It is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "What is the majority class in the **train_data**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112164\n",
      "21252\n"
     ]
    }
   ],
   "source": [
    "num_positive  = (train_data['sentiment'] == +1).sum()\n",
    "num_negative = (train_data['sentiment'] == -1).sum()\n",
    "print(num_positive)\n",
    "print(num_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the accuracy of the majority class classifier on **test_data**.\n",
    "\n",
    "**Quiz Question**: Enter the accuracy of the majority class classifier model on the **test_data**. Round your answer to two decimal places (e.g. 0.76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28095\n",
      "5241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8427825773938085"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_positive  = (test_data['sentiment'] == +1).sum()\n",
    "num_negative = (test_data['sentiment'] == -1).sum()\n",
    "print(num_positive)\n",
    "print(num_negative)\n",
    "num_positive/len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Is the **sentiment_model** definitely better than the majority class classifier (the baseline)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
